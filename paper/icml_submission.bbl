\begin{thebibliography}{13}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Cobbe et~al.(2021)]{cobbe2021training}
Cobbe, K. et~al.
\newblock Training verifiers to solve math word problems.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2021.
\newblock Please replace with full, accurate bibliographic details if
  available.

\bibitem[Gao et~al.(2023)Gao, Madaan, Zhou, Alon, Liu, Yang, and
  Neubig]{gao2023pal}
Gao, L., Madaan, A., Zhou, Y., Alon, U., Liu, P., Yang, Y., and Neubig, G.
\newblock Pal: Program-aided language models.
\newblock \emph{arXiv preprint arXiv:2211.10435}, 2023.

\bibitem[Kojima et~al.(2022)Kojima, Gu, Reid, Matsuo, and
  Iwasawa]{kojima2022letsthink}
Kojima, T., Gu, S., Reid, M., Matsuo, Y., and Iwasawa, Y.
\newblock Large language models are zero-shot reasoners.
\newblock \emph{arXiv preprint arXiv:2205.11916}, 2022.

\bibitem[Nye et~al.(2021)Nye, Andreas, Gur-Ari, Tenenbaum, Lake, and
  Linzen]{nye2021showyourwork}
Nye, M., Andreas, J., Gur-Ari, G., Tenenbaum, J.~B., Lake, B.~M., and Linzen,
  T.
\newblock Show your work: Scratchpads for intermediate computation with
  language models.
\newblock \emph{arXiv preprint arXiv:2112.00114}, 2021.

\bibitem[Schick et~al.(2023)Schick, Dwivedi-Yu, Dess{\`i}, Raunak, Havrilla,
  Scialom, Chen, Niehues, Schulte, Zettlemoyer, Schmid, and
  Petroni]{schick2023toolformer}
Schick, T., Dwivedi-Yu, J., Dess{\`i}, R., Raunak, V., Havrilla, A., Scialom,
  T., Chen, X., Niehues, J., Schulte, S., Zettlemoyer, L., Schmid, M., and
  Petroni, F.
\newblock Toolformer: Language models can teach themselves to use tools.
\newblock \emph{arXiv preprint arXiv:2302.04761}, 2023.

\bibitem[Shen et~al.(2023)Shen, Liu, Chen, Xu, Wang, Zhang, Liang, Huang, Chen,
  and Zhang]{shen2023HuggingGPT}
Shen, Y., Liu, K., Chen, W., Xu, X., Wang, F., Zhang, Z., Liang, X., Huang, J.,
  Chen, Z., and Zhang, Y.
\newblock Hugginggpt: Solving ai tasks with chatgpt and its friends in hugging
  face.
\newblock \emph{arXiv preprint arXiv:2303.17580}, 2023.

\bibitem[Shinn et~al.(2023)Shinn, Labash, and Madaan]{shinn2023reflexion}
Shinn, N., Labash, T., and Madaan, A.
\newblock Reflexion: Language agents with verbal reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2303.11366}, 2023.

\bibitem[{Significant-Gravitas}(2023)]{autogpt2023}
{Significant-Gravitas}.
\newblock Autogpt.
\newblock \url{https://github.com/Significant-Gravitas/AutoGPT}, 2023.
\newblock Accessed 2025-10-05.

\bibitem[Wang et~al.(2022)Wang, Wei, Schuurmans, Le, Chi, and
  Zhou]{wang2022selfconsistency}
Wang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E.~H., and Zhou, D.
\newblock Self-consistency improves chain of thought reasoning in language
  models.
\newblock \emph{arXiv preprint arXiv:2203.11171}, 2022.

\bibitem[Wei et~al.(2022)Wei, Wang, Schuurmans, Bosma, Ichter, Xia, Chi, Le,
  and Zhou]{wei2022chain}
Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia, F., Chi, E., Le,
  Q., and Zhou, D.
\newblock Chain-of-thought prompting elicits reasoning in large language
  models.
\newblock \emph{arXiv preprint arXiv:2201.11903}, 2022.

\bibitem[Yao et~al.(2023{\natexlab{a}})Yao, Yu, Zhao, Shafran, Welleck, and
  Radev]{yao2023tree}
Yao, S., Yu, D., Zhao, J., Shafran, I., Welleck, S., and Radev, D.
\newblock Tree of thoughts: Deliberate problem solving with large language
  models.
\newblock \emph{arXiv preprint arXiv:2305.10601}, 2023{\natexlab{a}}.

\bibitem[Yao et~al.(2023{\natexlab{b}})Yao, Zhao, Yu, Wang, and
  Radev]{yao2023react}
Yao, S., Zhao, J., Yu, D., Wang, N.~D., and Radev, D.
\newblock React: Synergizing reasoning and acting in language models.
\newblock \emph{arXiv preprint arXiv:2210.03629}, 2023{\natexlab{b}}.

\bibitem[{Yohei Nakajima}(2023)]{babyagi2023}
{Yohei Nakajima}.
\newblock Babyagi.
\newblock \url{https://github.com/yoheinakajima/babyagi}, 2023.
\newblock Accessed 2025-10-05.

\end{thebibliography}
